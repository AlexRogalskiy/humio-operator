apiVersion: core.humio.com/v1alpha1
kind: HumioCluster
metadata:
  name: example-humiocluster
  namespace: default
spec:
  targetReplicationFactor: 2
  storagePartitionsCount: 24
  digestPartitionsCount: 24
  hostname: my-cluster.example.com
  esHostname: my-cluster-es.example.com
  tls:
    enabled: true
  ingress:
    enabled: true
    controller: nginx # alb, traefik, contour, openshift-router, (nlb?)
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      kubernetes.io/ingress.class: nginx
      use-http01-solver: "true"
      serviceRoleMappings:
        # Official docs lists quite a few ingress controllers: https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
        # Fairly old comparison of ingress controllers: https://medium.com/flant-com/comparing-ingress-controllers-for-kubernetes-9b397483b46b
        # Notes about some of them:
        # - kubernetes/ingress-nginx
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#ingressspec-v1beta1-networking-k8s-io
        #     It accepts overlapping hostnames, but if combined hostname+path match the first object created wins (how about updates?):
        #     Nginx Inc has their own separate nginx ingress controller from this one.
        # - HAProxy
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://github.com/haproxytech/kubernetes-ingress/blob/master/documentation/README.md
        # - AKS
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://azure.github.io/application-gateway-kubernetes-ingress/
        # - GKE
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer and https://github.com/kubernetes/ingress-gce
        # - Voyager
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://voyagermesh.com/docs/v12.0.0/concepts/ingress-types/loadbalancer/
        # - Citrix
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://github.com/citrix/citrix-k8s-ingress-controller/blob/master/docs/configure/annotations.md
        # - AWS ALB ingress controller
        #     Can use "Ingress" objects to provision ALB's, but can create NLB's based on Service objects: https://github.com/kubernetes-sigs/aws-load-balancer-controller
        #     Service -> NLB might not require use of HumioCluster.Spec.Ingress.* but instead rely on HumioCluster.Spec.ServiceRoles.* to set annotations, depending on how it works.

        # - Traefik
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://doc.traefik.io/traefik/routing/providers/kubernetes-ingress/
        #     Can use "IngressRoute" which is their own CRD: https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/
        # - Skipper
        #     Can use "Ingress" objects with additional annotations for fine-grained configuration: https://opensource.zalando.com/skipper/kubernetes/ingress-usage/
        #     Can use "RouteGroup" which is their own CRD: https://opensource.zalando.com/skipper/kubernetes/routegroups/
        # - Istio
        #     Can use "Ingress" objects, but does not seem to allow additional configuration: https://istio.io/latest/docs/reference/config/annotations/ and https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/
        #     Can use "VirtualService" and "Gateway" with is their own CRD's for fine-grained configuration: https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/

        # - OpenShift
        #     Can use "Route" objects which is their own CRD: https://docs.openshift.com/container-platform/4.6/rest_api/network_apis/route-route-openshift-io-v1.html
        # - Contour
        #     Can use "HTTPProxy" which is their own CRD but previously used "IngressRoute" objects: https://github.com/projectcontour/contour/blob/main/site/docs/v1.9.0/httpproxy.md
        #     Contour 1.6 (2020-06-26) removed support for "IngressRoute".
        # - Ambassador
        #     Can use "Mapping" and "Host" which is their own CRD's: https://www.getambassador.io/docs/latest/tutorials/quickstart-demo/
        # - Kong
        #     Can use "KongIngress" which is their own CRD: https://github.com/Kong/kubernetes-ingress-controller/blob/ed88ebee538681888f303c572c139410f4d686f8/pkg/client/configuration/clientset/versioned/typed/configuration/v1/kongingress.go
        # - Gloo
        #     Can use "VirtualService" and "Upstream" which is their own CRD's: https://docs.solo.io/gloo-edge/latest/introduction/traffic_management/ and https://docs.solo.io/gloo-edge/latest/guides/traffic_management/hello_world/
        stateful:
        - /
        ingest:
        - /
        # HOW ABOUT MULTIPLE PORTS?
        # We currently have 8080 and 9200, but we might want additional ports for on-prem purposes for e.g. netflow.
        # There's also situations where you do not need the ES hostname, so can we drop that one if not needed?
        # Do we want to have a multi-port service implementation or should this be a one-service-per-port situation?
        # Option 1: Expose/configure all ports on all services and rely on pod selector to send traffic to only relevant pods.
        #           This might cause problems if you want *only* the ES port available on a given service?
        #           Or if you only want to expose port 8080 with "type: Loadbalancer" while not configuring any other port like 9200.
        # Option 2: Create a service per "service role" per port, so essentially ending up with this many services for a given cluster: #ports * #nodePools

        # Scenarios:
        # 1. Random Layer4 loadbalancer -> ingress-nginx (TLS) -> Humio (TLS)
        # 2. Random Layer4 loadbalancer -> ingress-nginx (TLS) -> Humio (no TLS)
        # 3. Random Layer4 loadbalancer -> ingress-nginx (no TLS) -> Humio (no TLS)
        #    In this scenario, the l4 loadbalancer might be AWS ELB/NLB/ALB which can terminate TLS.
        # 3. Random Layer4 loadbalancer -> Humio (no TLS)
        #    Some customers might prefer this for air-gapped setups, or accept the risk by terminating the TLS connection early at e.g. the AWS NLB (with certs from AWS ACM).
        # 4. Random Layer4 loadbalancer -> Humio (TLS)
        #    This could rely on certificates obtained using things like cert-manager, or manually provided.
  serviceRoles:
    stateful:
      targetPort: 8080 # TODO: This will probably prevent us from baking in support for single-service with multiple ports configured, which would be nice for scenarios that doesn't need ingress but still wants type LoadBalancer Service to expose both port 443->8080 and 9200->9200
      type: LoadBalancer
      labels:
        purpose: "logging"
      annotations:
        "service.beta.kubernetes.io/aws-load-balancer-type"                              : "nlb"
        "service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled" : "false"
        "service.beta.kubernetes.io/aws-load-balancer-ssl-cert"                          : "arn:aws:acm:region:account:certificate/123456789012-1234-1234-1234-12345678"
        "service.beta.kubernetes.io/aws-load-balancer-backend-protocol"                  : "ssl"
        "service.beta.kubernetes.io/aws-load-balancer-ssl-ports"                         : "443"
        "service.beta.kubernetes.io/aws-load-balancer-internal"                          : "0.0.0.0/0"
    ingest: {}
  nodePools:
  - name: stateful
    attachedServiceRoles: ["stateful"]
    image: "humio/humio-core:1.18.1"
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: humio_node_type
              operator: In
              values:
              - core
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - humio
          topologyKey: kubernetes.io/hostname
    dataVolumeSource:
      hostPath:
        path: "/mnt/disks/vol1"
        type: "Directory"
    environmentVariables:
    - name: S3_STORAGE_BUCKET
      value: "my-cluster-storage"
    - name: S3_STORAGE_REGION
      value: "us-west-2"
    - name: S3_STORAGE_ENCRYPTION_KEY
      value: "my-encryption-key"
    - name: USING_EPHEMERAL_DISKS
      value: "true"
    - name: S3_STORAGE_PREFERRED_COPY_SOURCE
      value: "true"
    - name: HUMIO_JVM_ARGS
      value: -Xss2m -Xms2g -Xmx26g -server -XX:MaxDirectMemorySize=26g -XX:+UseParallelOldGC -XX:+UnlockDiagnosticVMOptions -XX:CompileCommand=dontinline,com/humio/util/HotspotUtilsJ.dontInline -Xlog:gc+jni=debug:stdout -Dakka.log-config-on-start=on -Xlog:gc*:stdout:time,tags
    - name: "ZOOKEEPER_URL"
      value: "z-2-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181,z-3-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181,z-1-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181"
    - name: "KAFKA_SERVERS"
      value: "b-2-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092,b-1-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092,b-3-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092"
  - name: ingest
    attachedServiceRoles: ["ingest"]
    image: "humio/humio-core:1.18.1"
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: humio_node_type
              operator: In
              values:
              - core
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - humio
          topologyKey: kubernetes.io/hostname
    dataVolumeSource:
      hostPath:
        path: "/mnt/disks/vol1"
        type: "Directory"
    environmentVariables:
      - name: S3_STORAGE_BUCKET
        value: "my-cluster-storage"
      - name: S3_STORAGE_REGION
        value: "us-west-2"
      - name: S3_STORAGE_ENCRYPTION_KEY
        value: "my-encryption-key"
      - name: USING_EPHEMERAL_DISKS
        value: "true"
      - name: S3_STORAGE_PREFERRED_COPY_SOURCE
        value: "true"
      - name: HUMIO_JVM_ARGS
        value: -Xss2m -Xms2g -Xmx26g -server -XX:MaxDirectMemorySize=26g -XX:+UseParallelOldGC -XX:+UnlockDiagnosticVMOptions -XX:CompileCommand=dontinline,com/humio/util/HotspotUtilsJ.dontInline -Xlog:gc+jni=debug:stdout -Dakka.log-config-on-start=on -Xlog:gc*:stdout:time,tags
      - name: "ZOOKEEPER_URL"
        value: "z-2-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181,z-3-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181,z-1-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:2181"
      - name: "KAFKA_SERVERS"
        value: "b-2-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092,b-1-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092,b-3-my-zookeeper.c4.kafka.us-west-2.amazonaws.com:9092"